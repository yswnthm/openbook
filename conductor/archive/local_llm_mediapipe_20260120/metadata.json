{
  "track_id": "local_llm_mediapipe_20260120",
  "type": "feature",
  "status": "new",
  "created_at": "2026-01-20T12:00:00Z",
  "updated_at": "2026-01-20T12:00:00Z",
  "description": "Run LLM Inference Engine through NodeJS/WebGPU using MediaPipe LiteRT for offline chat."
}
